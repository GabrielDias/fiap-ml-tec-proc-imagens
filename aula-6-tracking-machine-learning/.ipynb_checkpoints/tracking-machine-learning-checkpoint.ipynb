{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rastreamento de objetos e machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rastreamento de cores, objetos e aplicação de técnicas de machine learning pré-treinadas e desenvolvimento de algoritmos próprios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Importante ressaltar que precisamos do OpenCV >= 3.4.2. Pode ser instalado pelo comando**\n",
    "\n",
    "```conda install -c conda-forge opencv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando versão instalada do OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastreamento de objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtro por cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibração de cor. \n",
    "\n",
    "É importante ressaltar que o Gimp usa o HSV com valores H de 0 a 180 S e V de 0 a 360. O OpenCV utiliza H de 0 a 100 e S e V de 0 a 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[120 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "blue = np.uint8([[[255, 0, 0]]])\n",
    "hsv = cv2.cvtColor(blue,cv2.COLOR_BGR2HSV)\n",
    "print(hsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando uma imagem estática.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo limiares de cor, adotando H-10, 100, 100 para limite inferior e H+10, 255, 255 para limite superior\n",
    "lower_blue = np.array([100,100,100])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "imagem = cv2.imread(\"imagens/caneta.jpg\")\n",
    "imagem = cv2.resize(imagem, (0,0), fx=0.5, fy=0.5) \n",
    "\n",
    "#Convertendo para HSV\n",
    "hsv_img = cv2.cvtColor(imagem, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Criação de máscara de cores dentro do limtie\n",
    "mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "\n",
    "# Operação AND entre a máscara e a imagem original\n",
    "res = cv2.bitwise_and(imagem, imagem, mask=mask)\n",
    "\n",
    "cv2.imshow('Imagem original', imagem)  \n",
    "cv2.imshow('Mascara', mask)\n",
    "cv2.imshow('Filtro de cor azul', res)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando captura contínua de uma webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definindo limiares de cor, adotando H-10, 100, 100 para limite inferior e H+10, 255, 255 para limite superior\n",
    "lower_blue = np.array([100,100,100])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Os filtros foram feitos com HSV, logo precisamos converter neste spaço de cores.\n",
    "        hsv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Criação de máscara de cores dentro do limtie\n",
    "        mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "\n",
    "        # Operação AND entre a máscara e a imagem original\n",
    "        res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "        cv2.imshow('Imagem original', frame)  \n",
    "        cv2.imshow('Mascara', mask)\n",
    "        cv2.imshow('Filtro de cor azul', res)\n",
    "        \n",
    "    #Tecla Enter interrompe o loop\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definindo limiares de cor, adotando H-10, 100, 100 para limite inferior e H+10, 255, 255 para limite superior\n",
    "lower_blue = np.array([100,100,100])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "upper_left = (50, 50)\n",
    "bottom_right = (300, 300)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        \n",
    "        #Rectangle marker\n",
    "        r = cv2.rectangle(frame, upper_left, bottom_right, (100, 50, 200), 5)\n",
    "        rect_img = frame[upper_left[1] : bottom_right[1], upper_left[0] : bottom_right[0]]\n",
    "        \n",
    "        hsv_img = cv2.cvtColor(rect_img, cv2.COLOR_BGR2HSV)\n",
    "        mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "        res = cv2.bitwise_and(rect_img, rect_img, mask=mask)\n",
    "        \n",
    "        frame[upper_left[1] : bottom_right[1], upper_left[0] : bottom_right[0]] = res\n",
    "\n",
    "        cv2.imshow('Imagem original', frame)  \n",
    "        \n",
    "    #Tecla Enter interrompe o loop\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rastreamento de objetos baseado em cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando um ROI diretamente na imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Obtendo o primeiro frame para posicionar o retângulo inicial\n",
    "\n",
    "cap = cv2.VideoCapture(\"videos/soccer-ita.mp4\")\n",
    "play = True\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"Extracao ROI\", frame)\n",
    "        cv2.imwrite(\"roi_sample.png\", frame)\n",
    "        break\n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolha do ROI da imagem, neste exemplo vamos focar no juiz de vermelho\n",
    "\n",
    "fromCenter = False\n",
    "frame = cv2.imread(\"roi_sample.png\")\n",
    "r = cv2.selectROI(\"Image\", frame, fromCenter)\n",
    "roi = frame[r[1]:(r[1]+r[3]), r[0]:(r[0]+r[2])]\n",
    "track_window = r[0], r[1], r[2], r[3]\n",
    "cv2.imwrite(\"roi_sample_defined.png\", roi)\n",
    "cv2.imshow(\"Extracao ROI\", roi)\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_window = r[0], r[1], r[2], r[3]\n",
    "\n",
    "# Conversão para o espaço HSV\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Cálculo do histograma\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0,180])\n",
    "\n",
    "# Normalizar os valors para o range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Critério de término, encerrará o cálculo do centroid após 10 interações\n",
    "# ou se o centróide mover, pelo menos 1 pixel\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 1)\n",
    "\n",
    "cap = cv2.VideoCapture(\"videos/soccer-ita.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Cálculo do histograma da projeção da imagem em região adjacente\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # Aplicação da técnica Mean Shift na nova região\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Incluir retângulo\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w, y+h), 255, 2)    \n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "        time.sleep(0.01)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando Meanshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Configuração do retângulo para capturar as características do objeto a ser rastreado\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.rectangle(frame, (c,r), (c+w, r+h), 255, 2)  \n",
    "        cv2.imshow(\"Extracao ROI\", frame)\n",
    "        if cv2.waitKey(1) == 13:\n",
    "            break\n",
    "\n",
    "# Recortar a região de interesse (ROI)\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Conversão para o espaço HSV\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Cálculo do histograma\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0,180])\n",
    "\n",
    "# Normalizar os valors para o range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Critério de término, encerrará o cálculo do centroid após 10 interações\n",
    "# ou se o centróide mover, pelo menos 1 pixel\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Cálculo do histograma da projeção da imagem em região adjacente\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # Aplicação da técnica Mean Shift na nova região\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Incluir retângulo\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w, y+h), 255, 2)    \n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconhecimento de caracteres (OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento de classificador baseado em uma imagem de dígitos numéricos de 0 a 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de cada célula: (50, 100, 20, 20)\n",
      "Amostras para treino 3500\n",
      "Amostras para teste 1500\n",
      "Precisão é de 93.47 %\n"
     ]
    }
   ],
   "source": [
    "# Obtendo imagem com números escritos a mão\n",
    "image = cv2.imread('imagens/digits.png')\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convertendo em imagem menor, apenas para visualização\n",
    "small = cv2.pyrDown(image)\n",
    "cv2.imshow('Digits Image', small)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Dividir a imagem em 5000 células, cada uma com tamanho de 20x20\n",
    "# Resulta em um array de 4 dimensões: 50 x 100 x 20 x 20\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    "\n",
    "# Convertendo em um array do Numpy (50,100,20,20)\n",
    "x = np.array(cells)\n",
    "print (\"Shape de cada célula: \" + str(x.shape))\n",
    "\n",
    "# Divisão de 70% de treinamento e 30% de testes\n",
    "# Valor -1 do reshape indica que os valores serão armazenados em uma única linha\n",
    "# Neste caso imagine que a matriz de cada imagem de 20 linhas x 20 colunas será convertido em uma única linha de 400 colunas\n",
    "train = x[:,:70].reshape(-1,400).astype(np.float32) # Size = (3500,400)\n",
    "test = x[:,70:100].reshape(-1,400).astype(np.float32) # Size = (1500,400)\n",
    "\n",
    "print(\"Amostras para treino \" + str(len(train)))\n",
    "print(\"Amostras para teste \" + str(len(test)))\n",
    "\n",
    "# Labels para dados de teste\n",
    "k = [0,1,2,3,4,5,6,7,8,9]\n",
    "      \n",
    "train_labels = np.repeat(k,350)[:,np.newaxis]\n",
    "test_labels = np.repeat(k,150)[:,np.newaxis]\n",
    "\n",
    "# Inicializando o KNN, treinando os dados e configurando com k (mínimo de vizinhos) = 3\n",
    "knn = cv2.ml.KNearest_create()\n",
    "knn.train(train, cv2.ml.ROW_SAMPLE, train_labels)\n",
    "ret, result, neighbors, distance = knn.findNearest(test, k=3)\n",
    "\n",
    "# Obtendo a validação do treino, comparando os resultados com os dados de teste\n",
    "matches = result == test_labels\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = 100 * correct/result.size\n",
    "\n",
    "print(\"Precisão é de %.2f\" % accuracy + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando funções para identificar um dígito a partir de uma nova imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def centroid_contour(contour):\n",
    "    # Obtem centróide X\n",
    "    M = cv2.moments(contour)\n",
    "    cx = int(M[\"m10\"]/M[\"m00\"])\n",
    "    return cx\n",
    "\n",
    "def make_square(image):\n",
    "    # Transforma uma imagem no formato quadrado (dimensões iguais)\n",
    "    black = [0,0,0]\n",
    "    img_dim = image.shape\n",
    "    height = img_dim[0]\n",
    "    width = img_dim[1]\n",
    "\n",
    "    if (height == width):\n",
    "        square = not_square\n",
    "        return square\n",
    "    else:\n",
    "        doublesize = cv2.resize(image,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n",
    "        height = height * 2\n",
    "        width = width * 2\n",
    "        if (height > width):\n",
    "            pad = int((height - width)/2)\n",
    "            doublesize_square = cv2.copyMakeBorder(doublesize,0,0,pad,pad,cv2.BORDER_CONSTANT,value=black)\n",
    "        else:\n",
    "            pad = int((width - height)/2)\n",
    "            doublesize_square = cv2.copyMakeBorder(doublesize,pad,pad,0,0, cv2.BORDER_CONSTANT,value=black)\n",
    "    \n",
    "    doublesize_square_dim = doublesize_square.shape\n",
    "    return doublesize_square\n",
    "\n",
    "def resize_dimension(dimensions, image):\n",
    "    # Redimensionar imagem dada uma dimensão\n",
    "    black = [0,0,0]\n",
    "    buffer_pix = 4\n",
    "    dimensions  = dimensions - buffer_pix\n",
    "    squared = image\n",
    "    r = float(dimensions) / squared.shape[1]\n",
    "    dim = (dimensions, int(squared.shape[0] * r))\n",
    "    resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "    img_dim2 = resized.shape\n",
    "    height_r = img_dim2[0]\n",
    "    width_r = img_dim2[1]\n",
    "    \n",
    "    if (height_r > width_r):\n",
    "        resized = cv2.copyMakeBorder(resized,0,0,0,1,cv2.BORDER_CONSTANT,value=black)\n",
    "    if (height_r < width_r):\n",
    "        resized = cv2.copyMakeBorder(resized,1,0,0,0,cv2.BORDER_CONSTANT,value=black)\n",
    "    \n",
    "    p = 2\n",
    "    ReSizedImg = cv2.copyMakeBorder(resized,p,p,p,p,cv2.BORDER_CONSTANT,value=black)\n",
    "    img_dim = ReSizedImg.shape\n",
    "    height = img_dim[0]\n",
    "    width = img_dim[1]\n",
    "    \n",
    "    return ReSizedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Numero identificado 12345\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Carregando imagem para detecção\n",
    "image = cv2.imread('imagens/numeros.png')\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow(\"Imagem Original\", image)\n",
    "cv2.imshow(\"Imagem Escala de Cinza\", gray)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Suavizando a imagem para posterior detecção de borda por Canny \n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "cv2.imshow(\"Suavizada\", blurred)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "edged = cv2.Canny(blurred, 30, 150)\n",
    "cv2.imshow(\"Imagem Vazada para Detecao de Contornos\", edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Encontrando contornos\n",
    "im2, contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Ordenando contornos pela coordenada X\n",
    "contours = sorted(contours, key = centroid_contour, reverse = False)\n",
    "\n",
    "full_number = []\n",
    "\n",
    "for c in contours:\n",
    "    # Para cada contorno, desenhar um retângulo para capturar a escrita\n",
    "    (x, y, w, h) = cv2.boundingRect(c)    \n",
    "    cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "    cv2.imshow(\"Contornos\", image)\n",
    "    \n",
    "    # Validando tamanho da imagem\n",
    "    if w >= 5 and h >= 25:\n",
    "        roi = blurred[y:y + h, x:x + w]\n",
    "        ret, roi = cv2.threshold(roi, 127, 255,cv2.THRESH_BINARY_INV)\n",
    "        squared = make_square(roi)\n",
    "        final = resize_dimension(20, squared)\n",
    "        cv2.imshow(\"Imagem Padronizada 20 x 20\", final)\n",
    "        \n",
    "        final_array = final.reshape((1,400))\n",
    "        final_array = final_array.astype(np.float32)\n",
    "        \n",
    "        ret, result, neighbours, dist = knn.findNearest(final_array, k=1)\n",
    "        number = str(int(float(result[0])))\n",
    "        full_number.append(number)\n",
    "        \n",
    "        # Retângulo em volta do dígito e número identificado\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(image, number, (x , y + 155), cv2.FONT_ITALIC, 2, (120, 200, 210), 3)\n",
    "        print(number)\n",
    "        cv2.imshow(\"Imagem Final\", image)\n",
    "        cv2.waitKey(0) \n",
    "         \n",
    "cv2.imshow(\"Imagem Final com Digitos\", image)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()\n",
    "print (\"Numero identificado \" + ''.join(full_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador de Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando 100 exemplos de faces, utilizando a webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colega de amostras completado\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Extrator de faces\n",
    "def face_extractor(img):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.2, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Coletar 100 exemplos de um determinado rosto\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"Amostra\", frame)\n",
    "        if face_extractor(frame) is not None:\n",
    "            count += 1\n",
    "            face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            file_name_path = 'imagens/faces/michel/' + str(count) + '.jpg'\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "\n",
    "            # Put count on images and display live count\n",
    "            cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "            cv2.imshow('Rosto Normalizado', face)\n",
    "\n",
    "        if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Colega de amostras completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Carregando exemplos de arquivos previamente coletados\n",
    "data_path = 'imagens/faces/michel/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "training_data, labels = [], []\n",
    "\n",
    "# Lendo as imagens e associando a um label\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    training_data.append(images)\n",
    "    labels.append(0)\n",
    "\n",
    "# Criando uma matriz da lista de labels\n",
    "labels = np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "# Treinamento do modelo\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "#model = cv2.face.EigenFaceRecognizer_create()\n",
    "model.train(training_data, labels)\n",
    "\n",
    "print(\"Modelo treinado com sucesso.\")\n",
    "\n",
    "persons = {0: \"Michel\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 37.80342247155461)\n",
      "(0, 37.129869193612535)\n",
      "(0, 35.95824026269599)\n",
      "(0, 37.54566806118522)\n",
      "(0, 35.69426178052242)\n",
      "(0, 36.588113203478734)\n",
      "(0, 36.12464707307609)\n",
      "(0, 36.6273894652844)\n",
      "(0, 37.24257420089889)\n",
      "(0, 36.52384228812256)\n",
      "(0, 36.59731348040359)\n",
      "(0, 36.58879604707952)\n",
      "(0, 35.78544485598243)\n",
      "(0, 35.242861466921404)\n",
      "(0, 34.62693915336375)\n",
      "(0, 34.55442618746753)\n",
      "(0, 35.812343170663674)\n",
      "(0, 35.0399446546539)\n",
      "(0, 36.49707059617389)\n",
      "(0, 36.777341124032525)\n",
      "(0, 35.941615731814636)\n",
      "(0, 35.80435864939674)\n",
      "(0, 35.748192810029025)\n",
      "(0, 36.53423806548551)\n",
      "(0, 36.453459365063125)\n",
      "(0, 36.10451559504511)\n",
      "(0, 34.91809571133085)\n",
      "(0, 34.677522031633515)\n",
      "(0, 35.16344149723285)\n",
      "(0, 35.49195603730864)\n",
      "(0, 33.913304070919246)\n",
      "(0, 35.50815975605328)\n",
      "(0, 36.94323524056914)\n",
      "(0, 36.26453363247546)\n",
      "(0, 37.624780526557686)\n",
      "(0, 38.812922656421485)\n",
      "(0, 37.59552226561866)\n",
      "(0, 38.11657265529747)\n",
      "(0, 38.36710348374922)\n",
      "(0, 38.526673963123365)\n",
      "(0, 39.09806096506947)\n",
      "(0, 37.44375981343035)\n",
      "(0, 38.82748796970362)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.1, 5)\n",
    "    if faces is ():\n",
    "        return img, [], 0, 0\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi, x, y\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    image, face, x, y = face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        results = model.predict(face)\n",
    "        \n",
    "        print(results)\n",
    "        \n",
    "        if x > 0:\n",
    "            display_string = \"Dist. \" + str(int(results[1])) + ' ' + persons[results[0]] \n",
    "            cv2.putText(image, display_string, (x, y-20), cv2.FONT_HERSHEY_DUPLEX, 1, (255,120,150), 2)\n",
    "\n",
    "        if int(results[1]) < 40:\n",
    "            cv2.putText(image, \"Reconhecido com sucesso\", (x, y-50), cv2.FONT_HERSHEY_DUPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognition', image)\n",
    "        else:\n",
    "            cv2.putText(image, \"Nao reconhecido\", (250, 450), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image)\n",
    "    except:\n",
    "        cv2.putText(image, \"Rosto nao identificado\", (220, 120) , cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"Nao reconhecido\", (250, 450), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador Yolo (You Only Look Once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário baixar os pesos (modelo de deep-learning) neste link https://pjreddie.com/media/files/yolov3.weights e copiar para  pasta weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from darknet import Darknet\n",
    "\n",
    "# Configurações na rede neural YOLOv3\n",
    "cfg_file = 'cfg/yolov3.cfg'\n",
    "m = Darknet(cfg_file)\n",
    "\n",
    "# Pesos pré-treinados\n",
    "weight_file = 'weights/yolov3.weights'\n",
    "m.load_weights(weight_file)\n",
    "\n",
    "# Rótulos de classes\n",
    "namesfile = 'data/coco.names'\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topologia da rede neural da YOLOv3\n",
    "m.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho da figura\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregando imagem para classificar\n",
    "img = cv2.imread('./imagens/surf.jpg')\n",
    "\n",
    "# Convertendo para o espaço de cores RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionando imagem para ser compatível com a primeira camada da rede neural  \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Exibição das imagens\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Resized Image')\n",
    "plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patamar de NMS (Non-Maximum Supression)\n",
    "# Ajuste de sensibilidade de imagens com baixa luminosidade\n",
    "nms_thresh = 0.6\n",
    "\n",
    "# Patamar do IOU (Intersect of Union), indicador se o retângulo \n",
    "# de identificação de imagem foi adequadamente desenhado\n",
    "iou_thresh = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo tamnaho do gráfico\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregar imagem para classificação\n",
    "img = cv2.imread('imagens/surf.jpg')\n",
    "\n",
    "# Conversão para o espaço RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionamento para adatapção da primeira camada da rede neural \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Deteteção de objetos na imagem\n",
    "boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n",
    "\n",
    "# Objetos encontrados e nível de confiança\n",
    "print_objects(boxes, class_names)\n",
    "\n",
    "# Desenho no gráfico com os regângulos e rótulos\n",
    "plot_boxes(original_image, boxes, class_names, plot_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects(boxes, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
